{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library Yang Dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import kaiserord, lfilter, firwin, freqz\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import heartpy as hp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from numpy import savetxt\n",
    "from scipy.stats import entropy\n",
    "from math import log, e\n",
    "import os, sys\n",
    "from scipy.signal import butter, iirnotch, lfilter\n",
    "from scipy.stats import norm, kurtosis\n",
    "from scipy.stats import skew    \n",
    "import neurokit2 as nk\n",
    "from scipy.signal import find_peaks\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interval RR + Time Domain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signal_peaks(signal, minimum=0, maximum=None, freq=500):\n",
    "    dist = freq/2\n",
    "    r_peaks = find_peaks(signal,distance=dist,prominence=(minimum,maximum))\n",
    "    return r_peaks[0].tolist()\n",
    "\n",
    "def get_rr(r_peaks, to_sec=False,sample_rate=125):\n",
    "    rr_list = []\n",
    "    start_stop = []\n",
    "    for i in range(len(r_peaks)-2):\n",
    "        rr_list.append(r_peaks[i+1]-r_peaks[i])\n",
    "        start_stop.append([r_peaks[i],r_peaks[i+1]])\n",
    "    if (to_sec):\n",
    "        rr_list = np.divide(rr_list,sample_rate)\n",
    "    return rr_list, start_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca Dataset, Denoising, serta Ekstrak Fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file dataset/sehat/sehat1-nizar.csv\n",
      "104.19681620839364\n",
      "Using file dataset/sehat/sehat10-bagas.csv\n",
      "90.8777969018933\n",
      "Using file dataset/sehat/sehat11-fian.csv\n",
      "107.06319702602231\n",
      "Using file dataset/sehat/sehat12-faizalaryo.csv\n",
      "112.37458193979933\n",
      "Using file dataset/sehat/sehat13-dani.csv\n",
      "113.68421052631578\n",
      "Using file dataset/sehat/sehat14 -pradana.csv\n",
      "105.993690851735\n",
      "Using file dataset/sehat/sehat15-rifkun.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:5244: RuntimeWarning: Mean of empty slice.\n",
      "  result = super().mean(axis=axis, dtype=dtype, **kwargs)[()]\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3724: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.68852459016394\n",
      "Using file dataset/sehat/sehat16-Nirel.csv\n",
      "86.4345738295318\n",
      "Using file dataset/sehat/sehat17-Sesil.csv\n",
      "101.75438596491229\n",
      "Using file dataset/sehat/sehat18-Almer.csv\n",
      "101.75438596491229\n",
      "Using file dataset/sehat/sehat19-Aqil.csv\n",
      "95.57522123893806\n",
      "Using file dataset/sehat/sehat2-bintang.csv\n",
      "118.49901250822909\n",
      "Using file dataset/sehat/sehat20-thomi.csv\n",
      "116.19596541786744\n",
      "Using file dataset/sehat/sehat21-fuad.csv\n",
      "107.73834972210346\n",
      "Using file dataset/sehat/sehat22 - mahavira.csv\n",
      "69.97840172786177\n",
      "Using file dataset/sehat/sehat23 - tati.csv\n",
      "83.35745296671492\n",
      "Using file dataset/sehat/sehat24-Alvyn.csv\n",
      "124.4060475161987\n",
      "Using file dataset/sehat/sehat25-Ivan.csv\n",
      "108.85529157667388\n",
      "Using file dataset/sehat/sehat26-habibi.csv\n",
      "104.66717674062743\n",
      "Using file dataset/sehat/sehat27-faris.csv\n",
      "90.6474820143885\n",
      "Using file dataset/sehat/sehat28-guslal.csv\n",
      "69.80609418282549\n",
      "Using file dataset/sehat/sehat29-teddy.csv\n",
      "111.47540983606557\n",
      "Using file dataset/sehat/sehat3-sena.csv\n",
      "107.80399274047187\n",
      "Using file dataset/sehat/sehat30-bayu.csv\n",
      "70.93596059113301\n",
      "Using file dataset/sehat/sehat4-safril.csv\n",
      "77.53846153846153\n",
      "Using file dataset/sehat/sehat5-hisyam.csv\n",
      "94.59041731066462\n",
      "Using file dataset/sehat/sehat6-niko.csv\n",
      "78.68852459016394\n",
      "Using file dataset/sehat/sehat7-aflah.csv\n",
      "111.340206185567\n",
      "Using file dataset/sehat/sehat8-wahyu.csv\n",
      "106.45161290322581\n",
      "Using file dataset/sehat/sehat9-eric.csv\n",
      "106.93069306930694\n",
      "Using file dataset/pasien/pasien 2.csv\n",
      "94.46064139941691\n",
      "Using file dataset/pasien/pasien 3.csv\n",
      "70.0\n",
      "Using file dataset/pasien/pasien 4.csv\n",
      "109.45945945945948\n",
      "Using file dataset/pasien/pasien 5.csv\n",
      "76.19047619047619\n",
      "Using file dataset/pasien/pasien 6.csv\n",
      "63.96841066140177\n",
      "Using file dataset/pasien/pasien 7.csv\n",
      "107.59651307596512\n",
      "Using file dataset/pasien/pasien 8.csv\n",
      "93.99477806788511\n",
      "Using file dataset/pasien/Pasien10-Full.csv\n",
      "61.40724946695096\n",
      "Using file dataset/pasien/Pasien11-Full.csv\n",
      "108.51063829787233\n",
      "Using file dataset/pasien/Pasien12-Full.csv\n",
      "71.28712871287128\n",
      "Using file dataset/pasien/Pasien13-Full.csv\n",
      "74.48275862068967\n",
      "Using file dataset/pasien/Pasien14-Full.csv\n",
      "106.4039408866995\n",
      "Using file dataset/pasien/Pasien15-Full.csv\n",
      "83.72093023255815\n",
      "Using file dataset/pasien/Pasien16-Full.csv\n",
      "76.69773635153129\n",
      "Using file dataset/pasien/pasien17.csv\n",
      "133.56341673856772\n",
      "Using file dataset/pasien/pasien18.csv\n",
      "101.9076923076923\n",
      "Using file dataset/pasien/pasien19.csv\n",
      "114.37216338880485\n",
      "Using file dataset/pasien/pasien20.csv\n",
      "98.9205103042198\n",
      "Using file dataset/pasien/pasien21.csv\n",
      "126.84989429175475\n",
      "Using file dataset/pasien/pasien22.csv\n",
      "85.77810871183915\n",
      "Using file dataset/pasien/pasien23.csv\n",
      "80.0\n",
      "Using file dataset/pasien/pasien24.csv\n",
      "116.58031088082902\n",
      "Using file dataset/pasien/pasien25.csv\n",
      "82.99711815561959\n",
      "Using file dataset/pasien/pasien26.csv\n",
      "119.24478304074198\n",
      "Using file dataset/pasien/pasien27.csv\n",
      "81.81818181818181\n",
      "Using file dataset/pasien/pasien29.csv\n",
      "102.12765957446811\n",
      "Using file dataset/pasien/pasien30.csv\n",
      "87.12186689714778\n",
      "Using file dataset/pasien/Pasien9-Full.csv\n",
      "96.2566844919786\n"
     ]
    }
   ],
   "source": [
    "J=0 # jumlah file\n",
    "directory_path = 'dataset/sehat'\n",
    "for iter in range(0,2):\n",
    "    for x in os.listdir(directory_path):\n",
    "        if not x.lower().endswith('.csv'):\n",
    "            continue\n",
    "        J=J+1\n",
    "    directory_path = 'dataset/pasien'\n",
    "n = J #jumlah file\n",
    "m = 10\n",
    "FEAT = [] #bakal jadi Feature.csv\n",
    "for i in range(n): \n",
    "    FEAT.append([0] * m) #mengisi dengan angka 0 semua \n",
    "directory_path = 'dataset/sehat'\n",
    "J=-1\n",
    "K=0\n",
    "for iter in range(0,2):\n",
    "    for x in os.listdir(directory_path):\n",
    "        if not x.lower().endswith('.csv'):\n",
    "            continue\n",
    "        full_file_path = directory_path  +   '/'   + x\n",
    "        J=J+1\n",
    "        print ('Using file', full_file_path)\n",
    "        try:\n",
    "            dataraw = pd.read_csv(full_file_path,index_col='Timestamp', parse_dates=['Timestamp'])\n",
    "            dataset = pd.DataFrame(dataraw['Value']) #ambil kolom value dari setiap file\n",
    "        except:\n",
    "            dataraw = pd.read_csv(full_file_path,index_col='timestamp', parse_dates=['timestamp'])\n",
    "            dataset = pd.DataFrame(dataraw['values'])\n",
    "        x1=np.array(dataset)  #ubah jadi array, namanya x1\n",
    "        Dat=[]\n",
    "        Dat=[0 for i in range(x1.size)] #bikin array kosong isinya 0 semua sepajang array x1\n",
    "        for i in range(0,x1.size-1):\n",
    "            Dat[i]=max(x1[i]) #why pakai max?\n",
    "        \n",
    "        \n",
    "        # FIR Filter\n",
    "        enhanced = hp.enhance_peaks(Dat, iterations=2)\n",
    "        powerline=60\n",
    "        fs = 110\n",
    "        cutoff_low = 3\n",
    "        nyq_rate = fs / 2.0\n",
    "        width = 5.0/nyq_rate\n",
    "        N, beta = kaiserord(powerline, width)\n",
    "        cutoff_hz = cutoff_low\n",
    "        taps = firwin(N, cutoff_hz/nyq_rate, window=('kaiser', beta))\n",
    "        y_filtered = lfilter(taps, 1.0, enhanced)\n",
    "            \n",
    "        wd, m = hp.process(y_filtered, sample_rate=120)   \n",
    "        \n",
    "        # FEATURE EXTRACTION\n",
    "        try:\n",
    "\n",
    "            arr=[]\n",
    "            for measure in m.keys():        \n",
    "                arr.append(m[measure])\n",
    "                \n",
    "            peaks = find_signal_peaks(y_filtered,minimum=0.2)\n",
    "            rr, rr_startstop = get_rr(peaks,to_sec=True,sample_rate=500)\n",
    "            mean_ppi = np.mean(rr)\n",
    "            std_ppi = np.std(rr)\n",
    "            mean_signal = np.mean(y_filtered)\n",
    "            std_signal = np.std(y_filtered)\n",
    "            \n",
    "            FEAT[J][0] = arr[0]\n",
    "            FEAT[J][1] = arr[1]\n",
    "            FEAT[J][2] = arr[2]\n",
    "            FEAT[J][3] = arr[7]\n",
    "            FEAT[J][4] = arr[12]\n",
    "            FEAT[J][5] = mean_ppi\n",
    "            FEAT[J][6] = std_ppi\n",
    "            FEAT[J][7] = mean_signal\n",
    "            FEAT[J][8] = std_signal\n",
    "            FEAT[J][9] = K\n",
    "            print(FEAT[J][0])\n",
    "\n",
    "        except:\n",
    "            J=J-1\n",
    "        \n",
    "    directory_path = 'dataset/pasien'\n",
    "    K=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export hasil ekstraksi fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#bikin csv\n",
    "with open(\"Features(sk2).csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(FEAT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca Dataset hasil ekstraksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# import sklearn\n",
    "\n",
    "dataset = pd.read_csv('Features(sk2).csv', names=['bpm','ibi','sdnn', 'hr_mad','breathingrate','Mean PPI', 'Std PPI', 'Mean Signal','Std signal','label'])\n",
    "\n",
    "# data_uji = pd.read_csv('Featuree2.csv', names=['bpm', 'ibi', 'sdnn','sdsd','rmssd','pnn20','pnn50','hr_mad','sd1','sd2','s','sd1/sd2','breathingrate'])\n",
    "label = dataset['label']\n",
    "dataset = dataset.drop(columns='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bpm              0\n",
       "ibi              0\n",
       "sdnn             0\n",
       "hr_mad           0\n",
       "breathingrate    4\n",
       "Mean PPI         0\n",
       "Std PPI          0\n",
       "Mean Signal      0\n",
       "Std signal       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['breathingrate']=dataset['breathingrate'].fillna(dataset['breathingrate'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tahapan Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA TRAIN\n",
      "(40, 9)\n",
      "LABEL TRAIN\n",
      "(40,)\n",
      "DATA TEST\n",
      "(18, 9)\n",
      "LABEL TEST\n",
      "(18,)\n",
      "HASIL PREDIKSI\n",
      "[1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 1 1 1]\n",
      "The training accuracy is 1.0\n",
      "The test accuracy is 0.8888888888888888\n",
      "HASIL PREDIKSI\n",
      "[0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
      "Akurasi SVM 61.111111111111114  %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "# MACHINE LEARNING\n",
    "# SPLIT DATA 70% TRAIN, 30% DATA TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from numpy import random\n",
    "\n",
    "x = random.randint(100)\n",
    "\n",
    "# 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.30,random_state=2)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "print('DATA TRAIN')\n",
    "print(X_train.shape)\n",
    "print('LABEL TRAIN')\n",
    "print(y_train.shape)\n",
    "print('DATA TEST')\n",
    "print(X_test.shape)\n",
    "print('LABEL TEST')\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# Import the classifier from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: Define the classifier, and fit it to the data\n",
    "model = DecisionTreeClassifier(random_state=20)\n",
    "model.fit(X_train, y_train) \n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print('HASIL PREDIKSI')\n",
    "print(y_test_pred)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "cm2 = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print('The training accuracy is', train_accuracy)\n",
    "print('The test accuracy is', test_accuracy)\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('HASIL PREDIKSI')\n",
    "print(y_pred)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ac = accuracy_score(y_test,y_pred)\n",
    "print (\"Akurasi SVM\", ac*100,' %') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hasil Sensitifiti, Spesifisiti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778\n",
      "1.0\n",
      "0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "TP = cm2[1][1]\n",
    "\n",
    "TN = cm2[0][0]\n",
    "FP = cm2[0][1]\n",
    "FN = cm2[1][0]\n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "\n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "\n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "\n",
    "# calculate precision\n",
    "conf_precision = (TN / float(TN + FP))\n",
    "print(conf_specificity)\n",
    "print(conf_sensitivity)\n",
    "print(conf_specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interval RR + HRV Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(data):\n",
    "    bases = collections.Counter([tmp_base for tmp_base in data])\n",
    "    # define distribution\n",
    "    dist = [x/sum(bases.values()) for x in bases.values()]\n",
    " \n",
    "    # use scipy to calculate entropy\n",
    "    entropy_value = entropy(dist, base=2)\n",
    " \n",
    "    return entropy_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca Dataset, Denoising, Serta Ekstraksi Ciri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file dataset/sehat/sehat1-nizar.csv\n",
      "Using file dataset/sehat/sehat10-bagas.csv\n",
      "Using file dataset/sehat/sehat11-fian.csv\n",
      "Using file dataset/sehat/sehat12-faizalaryo.csv\n",
      "Using file dataset/sehat/sehat13-dani.csv\n",
      "Using file dataset/sehat/sehat14 -pradana.csv\n",
      "Using file dataset/sehat/sehat15-rifkun.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:5244: RuntimeWarning: Mean of empty slice.\n",
      "  result = super().mean(axis=axis, dtype=dtype, **kwargs)[()]\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3724: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file dataset/sehat/sehat16-Nirel.csv\n",
      "Using file dataset/sehat/sehat17-Sesil.csv\n",
      "Using file dataset/sehat/sehat18-Almer.csv\n",
      "Using file dataset/sehat/sehat19-Aqil.csv\n",
      "Using file dataset/sehat/sehat2-bintang.csv\n",
      "Using file dataset/sehat/sehat20-thomi.csv\n",
      "Using file dataset/sehat/sehat21-fuad.csv\n",
      "Using file dataset/sehat/sehat22 - mahavira.csv\n",
      "Using file dataset/sehat/sehat23 - tati.csv\n",
      "Using file dataset/sehat/sehat24-Alvyn.csv\n",
      "Using file dataset/sehat/sehat25-Ivan.csv\n",
      "Using file dataset/sehat/sehat26-habibi.csv\n",
      "Using file dataset/sehat/sehat27-faris.csv\n",
      "Using file dataset/sehat/sehat28-guslal.csv\n",
      "Using file dataset/sehat/sehat29-teddy.csv\n",
      "Using file dataset/sehat/sehat3-sena.csv\n",
      "Using file dataset/sehat/sehat30-bayu.csv\n",
      "Using file dataset/sehat/sehat4-safril.csv\n",
      "Using file dataset/sehat/sehat5-hisyam.csv\n",
      "Using file dataset/sehat/sehat6-niko.csv\n",
      "Using file dataset/sehat/sehat7-aflah.csv\n",
      "Using file dataset/sehat/sehat8-wahyu.csv\n",
      "Using file dataset/sehat/sehat9-eric.csv\n",
      "Using file dataset/pasien/pasien 2.csv\n",
      "Using file dataset/pasien/pasien 3.csv\n",
      "Using file dataset/pasien/pasien 4.csv\n",
      "Using file dataset/pasien/pasien 5.csv\n",
      "Using file dataset/pasien/pasien 6.csv\n",
      "Using file dataset/pasien/pasien 7.csv\n",
      "Using file dataset/pasien/pasien 8.csv\n",
      "Using file dataset/pasien/Pasien10-Full.csv\n",
      "Using file dataset/pasien/Pasien11-Full.csv\n",
      "Using file dataset/pasien/Pasien12-Full.csv\n",
      "Using file dataset/pasien/Pasien13-Full.csv\n",
      "Using file dataset/pasien/Pasien14-Full.csv\n",
      "Using file dataset/pasien/Pasien15-Full.csv\n",
      "Using file dataset/pasien/Pasien16-Full.csv\n",
      "Using file dataset/pasien/pasien17.csv\n",
      "Using file dataset/pasien/pasien18.csv\n",
      "Using file dataset/pasien/pasien19.csv\n",
      "Using file dataset/pasien/pasien20.csv\n",
      "Using file dataset/pasien/pasien21.csv\n",
      "Using file dataset/pasien/pasien22.csv\n",
      "Using file dataset/pasien/pasien23.csv\n",
      "Using file dataset/pasien/pasien24.csv\n",
      "Using file dataset/pasien/pasien25.csv\n",
      "Using file dataset/pasien/pasien26.csv\n",
      "Using file dataset/pasien/pasien27.csv\n",
      "Using file dataset/pasien/pasien29.csv\n",
      "Using file dataset/pasien/pasien30.csv\n",
      "Using file dataset/pasien/Pasien9-Full.csv\n"
     ]
    }
   ],
   "source": [
    "J=0 # jumlah file\n",
    "directory_path = 'dataset/sehat'\n",
    "for iter in range(0,2):\n",
    "    for x in os.listdir(directory_path):\n",
    "        if not x.lower().endswith('.csv'):\n",
    "            continue\n",
    "        J=J+1\n",
    "    directory_path = 'dataset/pasien'\n",
    "n = J #jumlah file\n",
    "m = 13\n",
    "FEAT = [] #bakal jadi Feature.csv\n",
    "for i in range(n): \n",
    "    FEAT.append([0] * m) #mengisi dengan angka 0 semua \n",
    "directory_path = 'dataset/sehat'\n",
    "J=-1\n",
    "K=0\n",
    "for iter in range(0,2):\n",
    "    for x in os.listdir(directory_path):\n",
    "        if not x.lower().endswith('.csv'):\n",
    "            continue\n",
    "        full_file_path = directory_path  +   '/'   + x\n",
    "        J=J+1\n",
    "        print ('Using file', full_file_path)\n",
    "        try:\n",
    "            dataraw = pd.read_csv(full_file_path,index_col='Timestamp', parse_dates=['Timestamp'])\n",
    "            dataset = pd.DataFrame(dataraw['Value']) #ambil kolom value dari setiap file\n",
    "        except:\n",
    "            dataraw = pd.read_csv(full_file_path,index_col='timestamp', parse_dates=['timestamp'])\n",
    "            dataset = pd.DataFrame(dataraw['values'])\n",
    "        x1=np.array(dataset)  #ubah jadi array, namanya x1\n",
    "        Dat=[]\n",
    "        Dat=[0 for i in range(x1.size)] #bikin array kosong isinya 0 semua sepajang array x1\n",
    "        for i in range(0,x1.size-1):\n",
    "            Dat[i]=max(x1[i]) #why pakai max?\n",
    "        \n",
    "        \n",
    "        # FIR Filter\n",
    "        enhanced = hp.enhance_peaks(Dat, iterations=2)\n",
    "        powerline=60\n",
    "        fs = 110\n",
    "        cutoff_low = 3\n",
    "        nyq_rate = fs / 2.0\n",
    "        width = 5.0/nyq_rate\n",
    "        N, beta = kaiserord(powerline, width)\n",
    "        cutoff_hz = cutoff_low\n",
    "        taps = firwin(N, cutoff_hz/nyq_rate, window=('kaiser', beta))\n",
    "        y_filtered = lfilter(taps, 1.0, enhanced)\n",
    "            \n",
    "        wd, m = hp.process(y_filtered, sample_rate=120)   \n",
    "        \n",
    "        # FEATURE EXTRACTION\n",
    "        try:\n",
    "\n",
    "            arr=[]\n",
    "            for measure in m.keys():        \n",
    "                arr.append(m[measure])\n",
    "            # Shannon entropy\n",
    "            ESH=shannon_entropy(y_filtered)\n",
    "            FEAT[J][0] = ESH\n",
    "            \n",
    "\n",
    "            # MAD\n",
    "            series = pd.Series(y_filtered)\n",
    "            MAD = series.mad()\n",
    "            # Kurtosis\n",
    "            KURT = kurtosis(y_filtered)\n",
    "            # Skewness\n",
    "            SKEW = skew(y_filtered)\n",
    "           \n",
    "            #vlf,lf,hf\n",
    "            info = nk.ppg_findpeaks(y_filtered)\n",
    "            peak = info[\"PPG_Peaks\"]\n",
    "            hrv_freq = nk.hrv_frequency(peak, sampling_rate=39, normalize=True)\n",
    "            VLF=hrv_freq['HRV_VLF'].values[0]\n",
    "            LF=hrv_freq['HRV_LF'].values[0]\n",
    "            HF=hrv_freq['HRV_HF'].values[0]\n",
    "            \n",
    "            FEAT[J][0] = arr[0]\n",
    "            FEAT[J][1] = arr[1]\n",
    "            FEAT[J][2] = arr[2]\n",
    "            FEAT[J][3] = arr[7]\n",
    "            FEAT[J][4] = arr[12]\n",
    "            FEAT[J][5] = ESH\n",
    "            FEAT[J][6] = MAD\n",
    "            FEAT[J][7] = KURT\n",
    "            FEAT[J][8] = SKEW\n",
    "            FEAT[J][9] = VLF\n",
    "            FEAT[J][10] = LF\n",
    "            FEAT[J][11] = HF\n",
    "            FEAT[J][12] = K\n",
    "\n",
    "        except:\n",
    "            J=J-1\n",
    "        \n",
    "    directory_path = 'dataset/pasien'\n",
    "    K=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eksport Hasil Ekstraksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#bikin csv\n",
    "with open(\"Features(sk22).csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(FEAT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca Dataset hasil ekstraksi fitur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# import sklearn\n",
    "\n",
    "dataset = pd.read_csv('Features(sk22).csv', names=['bpm','ibi','sdnn', 'hr_mad','breathingrate','ESH', 'MAD', 'KURT','SKEW','VLF','LF','HF','label'])\n",
    "\n",
    "# data_uji = pd.read_csv('Featuree2.csv', names=['bpm', 'ibi', 'sdnn','sdsd','rmssd','pnn20','pnn50','hr_mad','sd1','sd2','s','sd1/sd2','breathingrate'])\n",
    "label = dataset['label']\n",
    "dataset = dataset.drop(columns='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bpm              0\n",
       "ibi              0\n",
       "sdnn             0\n",
       "hr_mad           0\n",
       "breathingrate    4\n",
       "ESH              0\n",
       "MAD              0\n",
       "KURT             0\n",
       "SKEW             0\n",
       "VLF              2\n",
       "LF               0\n",
       "HF               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['breathingrate']=dataset['breathingrate'].fillna(dataset['breathingrate'].mean())\n",
    "dataset['VLF'] = dataset['VLF'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tahapan Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA TRAIN\n",
      "(40, 12)\n",
      "LABEL TRAIN\n",
      "(40,)\n",
      "DATA TEST\n",
      "(18, 12)\n",
      "LABEL TEST\n",
      "(18,)\n",
      "HASIL PREDIKSI\n",
      "[1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1]\n",
      "The training accuracy is 1.0\n",
      "The test accuracy is 0.9444444444444444\n",
      "HASIL PREDIKSI\n",
      "[0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0]\n",
      "Akurasi SVM 55.55555555555556  %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "# MACHINE LEARNING\n",
    "# SPLIT DATA 80% TRAIN, 20% DATA TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from numpy import random\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.30,random_state=32)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "print('DATA TRAIN')\n",
    "print(X_train.shape)\n",
    "print('LABEL TRAIN')\n",
    "print(y_train.shape)\n",
    "print('DATA TEST')\n",
    "print(X_test.shape)\n",
    "print('LABEL TEST')\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# Import the classifier from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: Define the classifier, and fit it to the data\n",
    "model = DecisionTreeClassifier(random_state=20)\n",
    "model.fit(X_train, y_train) \n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print('HASIL PREDIKSI')\n",
    "print(y_test_pred)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "cm2 = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print('The training accuracy is', train_accuracy)\n",
    "print('The test accuracy is', test_accuracy)\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('HASIL PREDIKSI')\n",
    "print(y_pred)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ac = accuracy_score(y_test,y_pred)\n",
    "print (\"Akurasi SVM\", ac*100,' %') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hasil Sensitifiti, spesifisiti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n",
      "1.0\n",
      "0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "TP = cm2[1][1]\n",
    "\n",
    "TN = cm2[0][0]\n",
    "FP = cm2[0][1]\n",
    "FN = cm2[1][0]\n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "\n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "\n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "\n",
    "# calculate precision\n",
    "conf_precision = (TN / float(TN + FP))\n",
    "print(conf_specificity)\n",
    "print(conf_sensitivity)\n",
    "print(conf_specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
