{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import kaiserord, lfilter, firwin, freqz\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import heartpy as hp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from numpy import savetxt\n",
    "from scipy.stats import entropy\n",
    "from math import log, e\n",
    "import os, sys\n",
    "from scipy.signal import butter, iirnotch, lfilter\n",
    "from scipy.stats import norm, kurtosis\n",
    "from scipy.stats import skew    \n",
    "import neurokit2 as nk\n",
    "from scipy.signal import find_peaks\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interval  RR + Time  Domain Features + HRV Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signal_peaks(arraynya, minimum=0, maximum=None, freq=500):\n",
    "    dist = freq/2\n",
    "    r_peaks = find_peaks(arraynya,distance=dist,prominence=(minimum,maximum))\n",
    "    return r_peaks[0].tolist()\n",
    "\n",
    "def get_rr(r_peaks, to_sec=False,sample_rate=125):\n",
    "    rr_list = []\n",
    "    start_stop = []\n",
    "    for i in range(len(r_peaks)-2):\n",
    "        rr_list.append(r_peaks[i+1]-r_peaks[i])\n",
    "        start_stop.append([r_peaks[i],r_peaks[i+1]])\n",
    "    if (to_sec):\n",
    "        rr_list = np.divide(rr_list,sample_rate)\n",
    "    return rr_list, start_stop\n",
    "\n",
    "def shannon_entropy(data):\n",
    "    bases = collections.Counter([tmp_base for tmp_base in data])\n",
    "    # define distribution\n",
    "    dist = [x/sum(bases.values()) for x in bases.values()]\n",
    " \n",
    "    # use scipy to calculate entropy\n",
    "    entropy_value = entropy(dist, base=2)\n",
    " \n",
    "    return entropy_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca Dataset, Denoising, serta ekstraksi ciri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file dataset/sehat/sehat1-nizar.csv\n",
      "atas 12.679919878518422\n",
      "atas 1.699077420506783\n",
      "Using file dataset/sehat/sehat10-bagas.csv\n",
      "atas 12.66755510704113\n",
      "atas 0.7716151417651285\n",
      "Using file dataset/sehat/sehat11-fian.csv\n",
      "atas 12.674412981837069\n",
      "atas 2.1634105160453068\n",
      "Using file dataset/sehat/sehat12-faizalaryo.csv\n",
      "atas 12.6686634231753\n",
      "atas 1.9019884102934947\n",
      "Using file dataset/sehat/sehat13-dani.csv\n",
      "atas 12.673750739438056\n",
      "atas 1.6857134250429995\n",
      "Using file dataset/sehat/sehat14 -pradana.csv\n",
      "atas 12.683871868622601\n",
      "atas 1.4763230190869658\n",
      "Using file dataset/sehat/sehat15-rifkun.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\numpy\\ma\\core.py:5244: RuntimeWarning: Mean of empty slice.\n",
      "  result = super().mean(axis=axis, dtype=dtype, **kwargs)[()]\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3724: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atas 12.681238411777807\n",
      "atas 1.6326377724653018\n",
      "Using file dataset/sehat/sehat16-Nirel.csv\n",
      "atas 12.759264045188791\n",
      "atas -0.3467005731001139\n",
      "Using file dataset/sehat/sehat17-Sesil.csv\n",
      "atas 12.963437998346288\n",
      "atas -0.08881717359594836\n",
      "Using file dataset/sehat/sehat18-Almer.csv\n",
      "atas 12.963437998346288\n",
      "atas -0.08881717359594836\n",
      "Using file dataset/sehat/sehat19-Aqil.csv\n",
      "atas 12.89500718036866\n",
      "atas 1.4513565719104917\n",
      "Using file dataset/sehat/sehat2-bintang.csv\n",
      "atas 12.694575537525983\n",
      "atas 0.7819848769141858\n",
      "Using file dataset/sehat/sehat20-thomi.csv\n",
      "atas 13.192755142640372\n",
      "atas 1.0957014401509415\n",
      "Using file dataset/sehat/sehat21-fuad.csv\n",
      "atas 12.912140856295725\n",
      "atas 1.5120257484203106\n",
      "Using file dataset/sehat/sehat22 - mahavira.csv\n",
      "atas 12.863992637170803\n",
      "atas 0.5736350309725092\n",
      "Using file dataset/sehat/sehat23 - tati.csv\n",
      "atas 12.75405236752893\n",
      "atas 0.7438635803980507\n",
      "Using file dataset/sehat/sehat24-Alvyn.csv\n",
      "atas 12.96470185742045\n",
      "atas -0.49898857046277706\n",
      "Using file dataset/sehat/sehat25-Ivan.csv\n",
      "atas 12.864766511423442\n",
      "atas -0.19421941565145873\n",
      "Using file dataset/sehat/sehat26-habibi.csv\n",
      "atas 13.195372207402745\n",
      "atas 1.7382201528405976\n",
      "Using file dataset/sehat/sehat27-faris.csv\n",
      "atas 12.674412981837069\n",
      "atas 0.7812808312815703\n",
      "Using file dataset/sehat/sehat28-guslal.csv\n",
      "atas 12.670656249118444\n",
      "atas 0.80970814915271\n",
      "Using file dataset/sehat/sehat29-teddy.csv\n",
      "atas 12.672646326124623\n",
      "atas 1.4104514645814317\n",
      "Using file dataset/sehat/sehat3-sena.csv\n",
      "atas 12.685843811426087\n",
      "atas 0.7831962746375922\n",
      "Using file dataset/sehat/sehat30-bayu.csv\n",
      "atas 12.685624839726355\n",
      "atas 1.0748526296372805\n",
      "Using file dataset/sehat/sehat4-safril.csv\n",
      "atas 12.670213636738564\n",
      "atas 1.3641671566219853\n",
      "Using file dataset/sehat/sehat5-hisyam.csv\n",
      "atas 12.68628165514443\n",
      "atas 0.09811594150524708\n",
      "Using file dataset/sehat/sehat6-niko.csv\n",
      "atas 12.676397887044763\n",
      "atas 1.4284490739540536\n",
      "Using file dataset/sehat/sehat7-aflah.csv\n",
      "atas 12.675074920385443\n",
      "atas 0.5044127655395787\n",
      "Using file dataset/sehat/sehat8-wahyu.csv\n",
      "atas 12.673750739438056\n",
      "atas 1.9415860210483915\n",
      "Using file dataset/sehat/sehat9-eric.csv\n",
      "atas 12.68518679659535\n",
      "atas 1.5039955731564656\n",
      "Using file dataset/pasien/pasien 2.csv\n",
      "atas 12.642503028780526\n",
      "atas -0.09267176839187359\n",
      "Using file dataset/pasien/pasien 3.csv\n",
      "atas 12.862831046898002\n",
      "atas 1.0579569966405196\n",
      "Using file dataset/pasien/pasien 4.csv\n",
      "atas 12.743966029574171\n",
      "atas 0.10854970132769913\n",
      "Using file dataset/pasien/pasien 5.csv\n",
      "atas 12.963257356633045\n",
      "atas 1.6461762493609318\n",
      "Using file dataset/pasien/pasien 6.csv\n",
      "atas 13.233769450446921\n",
      "atas 1.5189872930465462\n",
      "Using file dataset/pasien/pasien 7.csv\n",
      "atas 12.378023850925487\n",
      "atas -0.7695547722537032\n",
      "Using file dataset/pasien/pasien 8.csv\n",
      "atas 12.866506212226206\n",
      "atas 0.888222934919869\n",
      "Using file dataset/pasien/Pasien10-Full.csv\n",
      "atas 12.967406404311772\n",
      "atas 1.2537305902989468\n",
      "Using file dataset/pasien/Pasien11-Full.csv\n",
      "atas 12.75321674917895\n",
      "atas 0.6028784715669425\n",
      "Using file dataset/pasien/Pasien12-Full.csv\n",
      "atas 12.966505451905745\n",
      "atas 1.279288388813815\n",
      "Using file dataset/pasien/Pasien13-Full.csv\n",
      "atas 12.643405277115676\n",
      "atas 0.7889877447564524\n",
      "Using file dataset/pasien/Pasien14-Full.csv\n",
      "atas 12.968846758896294\n",
      "atas 1.5495837776710475\n",
      "Using file dataset/pasien/Pasien15-Full.csv\n",
      "atas 12.514467502819267\n",
      "atas -0.2957405826750416\n",
      "Using file dataset/pasien/Pasien16-Full.csv\n",
      "atas 12.377210530388554\n",
      "atas 0.2854349759491631\n",
      "Using file dataset/pasien/pasien17.csv\n",
      "atas 12.519144787997414\n",
      "atas 0.5749615151855595\n",
      "Using file dataset/pasien/pasien18.csv\n",
      "atas 12.376668061857673\n",
      "atas 2.462580303808004\n",
      "Using file dataset/pasien/pasien19.csv\n",
      "atas 12.963257356633045\n",
      "atas 2.077524340367136\n",
      "Using file dataset/pasien/pasien20.csv\n",
      "atas 12.755930741084399\n",
      "atas 0.20803567423121905\n",
      "Using file dataset/pasien/pasien21.csv\n",
      "atas 12.376668061857673\n",
      "atas 1.8896559753410331\n",
      "Using file dataset/pasien/pasien22.csv\n",
      "atas 12.963076692298594\n",
      "atas 0.25990446346640267\n",
      "Using file dataset/pasien/pasien23.csv\n",
      "atas 12.75634782551659\n",
      "atas 0.6996558175454181\n",
      "Using file dataset/pasien/pasien24.csv\n",
      "atas 12.757806672197228\n",
      "atas 0.7711288029649234\n",
      "Using file dataset/pasien/pasien25.csv\n",
      "atas 13.061539958530142\n",
      "atas 1.5169659944011342\n",
      "Using file dataset/pasien/pasien26.csv\n",
      "atas 12.762589665542567\n",
      "atas 0.3862151726132417\n",
      "Using file dataset/pasien/pasien27.csv\n",
      "atas 13.231521210875698\n",
      "atas 0.7583071973787437\n",
      "Using file dataset/pasien/pasien29.csv\n",
      "atas 13.522704120798252\n",
      "atas 0.8416223713618883\n",
      "Using file dataset/pasien/pasien30.csv\n",
      "atas 12.740202388767903\n",
      "atas 1.2682182481138309\n",
      "Using file dataset/pasien/Pasien9-Full.csv\n",
      "atas 12.965964610272081\n",
      "atas -0.08863849546430307\n"
     ]
    }
   ],
   "source": [
    "J=0 # jumlah file\n",
    "directory_path = 'dataset/sehat'\n",
    "for iter in range(0,2):\n",
    "    for x in os.listdir(directory_path):\n",
    "        if not x.lower().endswith('.csv'):\n",
    "            continue\n",
    "        J=J+1\n",
    "    directory_path = 'dataset/pasien'\n",
    "n = J #jumlah file\n",
    "m = 17\n",
    "FEAT = [] #bakal jadi Feature.csv\n",
    "for i in range(n): \n",
    "    FEAT.append([0] * m) #mengisi dengan angka 0 semua \n",
    "directory_path = 'dataset/sehat'\n",
    "J=-1\n",
    "K=0\n",
    "metoda=3\n",
    "for iter in range(0,2):\n",
    "    for x in os.listdir(directory_path):\n",
    "        if not x.lower().endswith('.csv'):\n",
    "            continue\n",
    "        full_file_path = directory_path  +   '/'   + x\n",
    "        J=J+1\n",
    "        print ('Using file', full_file_path)\n",
    "        try:\n",
    "            dataraw = pd.read_csv(full_file_path,index_col='Timestamp', parse_dates=['Timestamp'])\n",
    "            dataset = pd.DataFrame(dataraw['Value']) #ambil kolom value dari setiap file\n",
    "        except:\n",
    "            dataraw = pd.read_csv(full_file_path,index_col='timestamp', parse_dates=['timestamp'])\n",
    "            dataset = pd.DataFrame(dataraw['values'])\n",
    "        x1=np.array(dataset)  #ubah jadi array, namanya x1\n",
    "        Dat=[]\n",
    "        Dat=[0 for i in range(x1.size)] #bikin array kosong isinya 0 semua sepajang array x1\n",
    "        for i in range(0,x1.size-1):\n",
    "            Dat[i]=max(x1[i]) #why pakai max?\n",
    "        \n",
    "        \n",
    "        # FIR Filter\n",
    "        enhanced = hp.enhance_peaks(Dat, iterations=2)\n",
    "        powerline=60\n",
    "        fs = 110\n",
    "        cutoff_low = 3\n",
    "        nyq_rate = fs / 2.0\n",
    "        width = 5.0/nyq_rate\n",
    "        N, beta = kaiserord(powerline, width)\n",
    "        cutoff_hz = cutoff_low\n",
    "        taps = firwin(N, cutoff_hz/nyq_rate, window=('kaiser', beta))\n",
    "        y_filtered = lfilter(taps, 1.0, enhanced)\n",
    "            \n",
    "        wd, m = hp.process(y_filtered, sample_rate=120)\n",
    "        \n",
    "        \n",
    "        # FEATURE EXTRACTION\n",
    "        try:\n",
    "            #interval RR\n",
    "            arr=[]\n",
    "            for measure in m.keys():        \n",
    "                arr.append(m[measure])\n",
    "                \n",
    "            #HRV Features\n",
    "            #shannon entropy    \n",
    "            ESH=shannon_entropy(y_filtered)\n",
    "            FEAT[J][0] = ESH\n",
    "            print('atas',FEAT[J][0])\n",
    "            # MAD\n",
    "            series = pd.Series(y_filtered)\n",
    "            MAD = series.mad()\n",
    "            # Kurtosis\n",
    "            KURT = kurtosis(y_filtered)\n",
    "            # Skewness\n",
    "            SKEW = skew(y_filtered)\n",
    "            print('atas',SKEW)\n",
    "        \n",
    "            #vlf,lf,hf\n",
    "            info = nk.ppg_findpeaks(y_filtered)\n",
    "            peak = info[\"PPG_Peaks\"]\n",
    "            hrv_freq = nk.hrv_frequency(peak, sampling_rate=39, normalize=True)\n",
    "            VLF=hrv_freq['HRV_VLF'].values[0]\n",
    "            LF=hrv_freq['HRV_LF'].values[0]\n",
    "            HF=hrv_freq['HRV_HF'].values[0]\n",
    "            \n",
    "            #time domain features\n",
    "            peaks = find_signal_peaks(y_filtered,minimum=0.2)\n",
    "            rr, rr_startstop = get_rr(peaks,to_sec=True,sample_rate=500)\n",
    "            mean_ppi = np.mean(rr)\n",
    "            std_ppi = np.std(rr)\n",
    "            mean_signal = np.mean(y_filtered)\n",
    "            std_signal = np.std(y_filtered)\n",
    "            \n",
    "            FEAT[J][0] = arr[0]\n",
    "            FEAT[J][1] = arr[1]\n",
    "            FEAT[J][2] = arr[2]\n",
    "            FEAT[J][3] = arr[7]\n",
    "            FEAT[J][4] = arr[12]\n",
    "            FEAT[J][5] = mean_ppi\n",
    "            FEAT[J][6] = std_ppi\n",
    "            FEAT[J][7] = mean_signal\n",
    "            FEAT[J][8] = std_signal\n",
    "            FEAT[J][9] = ESH\n",
    "            FEAT[J][10] = MAD\n",
    "            FEAT[J][11] = KURT\n",
    "            FEAT[J][12] = SKEW\n",
    "            FEAT[J][13] = VLF\n",
    "            FEAT[J][14] = LF\n",
    "            FEAT[J][15] = HF\n",
    "            FEAT[J][16] = K\n",
    "        except:\n",
    "            J=J-1\n",
    "        \n",
    "    directory_path = 'dataset/pasien'\n",
    "    K=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Hasil ektraksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#bikin csv\n",
    "with open(\"Features(sk3).csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(FEAT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca Daset Hasil Ekstraksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# import sklearn\n",
    "\n",
    "dataset = pd.read_csv('Features(sk3).csv', names=['bpm','ibi','sdnn', 'hr_mad','breathingrate','Mean PPI', 'Std PPI', 'Mean Signal','Std signal','ESH', 'MAD', 'KURT','SKEW','VLF','LF','HF','label'])\n",
    "\n",
    "# data_uji = pd.read_csv('Featuree2.csv', names=['bpm', 'ibi', 'sdnn','sdsd','rmssd','pnn20','pnn50','hr_mad','sd1','sd2','s','sd1/sd2','breathingrate'])\n",
    "label = dataset['label']\n",
    "dataset = dataset.drop(columns='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bpm              0\n",
       "ibi              0\n",
       "sdnn             0\n",
       "hr_mad           0\n",
       "breathingrate    4\n",
       "Mean PPI         0\n",
       "Std PPI          0\n",
       "Mean Signal      0\n",
       "Std signal       0\n",
       "ESH              0\n",
       "MAD              0\n",
       "KURT             0\n",
       "SKEW             0\n",
       "VLF              2\n",
       "LF               0\n",
       "HF               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['breathingrate']=dataset['breathingrate'].fillna(dataset['breathingrate'].mean())\n",
    "dataset['VLF']=dataset['VLF'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tahapan Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "DATA TRAIN\n",
      "(40, 16)\n",
      "LABEL TRAIN\n",
      "(40,)\n",
      "DATA TEST\n",
      "(18, 16)\n",
      "LABEL TEST\n",
      "(18,)\n",
      "HASIL PREDIKSI\n",
      "[0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1]\n",
      "The training accuracy is 1.0\n",
      "The test accuracy is 0.9444444444444444\n",
      "HASIL PREDIKSI\n",
      "[0 1 1 0 1 0 0 1 0 1 0 0 0 1 1 0 1 1]\n",
      "Akurasi SVM 72.22222222222221  %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "# MACHINE LEARNING\n",
    "# SPLIT DATA 80% TRAIN, 20% DATA TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from numpy import random\n",
    "\n",
    "x = random.randint(100)\n",
    "print(x)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.30,random_state=39)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "print('DATA TRAIN')\n",
    "print(X_train.shape)\n",
    "print('LABEL TRAIN')\n",
    "print(y_train.shape)\n",
    "print('DATA TEST')\n",
    "print(X_test.shape)\n",
    "print('LABEL TEST')\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# Import the classifier from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: Define the classifier, and fit it to the data\n",
    "model = DecisionTreeClassifier(random_state=20)\n",
    "model.fit(X_train, y_train) \n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print('HASIL PREDIKSI')\n",
    "print(y_test_pred)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "cm2 = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print('The training accuracy is', train_accuracy)\n",
    "print('The test accuracy is', test_accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hasil Sensitifiti, Spesifisiti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "TP = cm2[1][1]\n",
    "\n",
    "TN = cm2[0][0]\n",
    "FP = cm2[0][1]\n",
    "FN = cm2[1][0]\n",
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "\n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "\n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "\n",
    "# calculate precision\n",
    "conf_precision = (TN / float(TN + FP))\n",
    "print(conf_specificity)\n",
    "print(conf_sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
