{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import kaiserord, lfilter, firwin, freqz\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "import collections\n",
    "from numpy import savetxt\n",
    "from scipy.stats import entropy\n",
    "from math import log, e\n",
    "import os, sys\n",
    "from scipy.signal import butter, iirnotch, lfilter\n",
    "from scipy.stats import norm, kurtosis\n",
    "from scipy.stats import skew    \n",
    "import neurokit2 as nk\n",
    "from scipy.signal import find_peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signal_peaks(arraynya, minimum=0, maximum=None, freq=500):\n",
    "    dist = freq/2\n",
    "    r_peaks = find_peaks(arraynya,distance=dist,prominence=(minimum,maximum))\n",
    "    return r_peaks[0].tolist()\n",
    "\n",
    "def get_rr(r_peaks, to_sec=False,sample_rate=125):\n",
    "    rr_list = []\n",
    "    start_stop = []\n",
    "    for i in range(len(r_peaks)-2):\n",
    "        rr_list.append(r_peaks[i+1]-r_peaks[i])\n",
    "        start_stop.append([r_peaks[i],r_peaks[i+1]])\n",
    "    if (to_sec):\n",
    "        rr_list = np.divide(rr_list,sample_rate)\n",
    "    return rr_list, start_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baca dataset, denoising, ekstraksi ciri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using file dataset/sehat/sehat1-nizar.csv\n",
      "1.1447999999999998\n",
      "Using file dataset/sehat/sehat10-bagas.csv\n",
      "0.9174999999999999\n",
      "Using file dataset/sehat/sehat11-fian.csv\n",
      "0.828\n",
      "Using file dataset/sehat/sehat12-faizalaryo.csv\n",
      "0.7754666666666666\n",
      "Using file dataset/sehat/sehat13-dani.csv\n",
      "1.6002857142857145\n",
      "Using file dataset/sehat/sehat14 -pradana.csv\n",
      "0.8816666666666667\n",
      "Using file dataset/sehat/sehat15-rifkun.csv\n",
      "0.9230909090909091\n",
      "Using file dataset/sehat/sehat16-Nirel.csv\n",
      "1.5154999999999998\n",
      "Using file dataset/sehat/sehat17-Sesil.csv\n",
      "1.4693333333333332\n",
      "Using file dataset/sehat/sehat18-Almer.csv\n",
      "1.4693333333333332\n",
      "Using file dataset/sehat/sehat19-Aqil.csv\n",
      "1.0869230769230769\n",
      "Using file dataset/sehat/sehat2-bintang.csv\n",
      "0.8452727272727274\n",
      "Using file dataset/sehat/sehat20-thomi.csv\n",
      "0.9181111111111111\n",
      "Using file dataset/sehat/sehat21-fuad.csv\n",
      "0.9201333333333334\n",
      "Using file dataset/sehat/sehat22 - mahavira.csv\n",
      "0.972\n",
      "Using file dataset/sehat/sehat23 - tati.csv\n",
      "0.9824999999999999\n",
      "Using file dataset/sehat/sehat24-Alvyn.csv\n",
      "1.9182857142857141\n",
      "Using file dataset/sehat/sehat25-Ivan.csv\n",
      "1.2993999999999999\n",
      "Using file dataset/sehat/sehat26-habibi.csv\n",
      "0.9287058823529412\n",
      "Using file dataset/sehat/sehat27-faris.csv\n",
      "1.36025\n",
      "Using file dataset/sehat/sehat28-guslal.csv\n",
      "1.0375\n",
      "Using file dataset/sehat/sehat29-teddy.csv\n",
      "0.993\n",
      "Using file dataset/sehat/sehat3-sena.csv\n",
      "1.302888888888889\n",
      "Using file dataset/sehat/sehat30-bayu.csv\n",
      "1.0977777777777777\n",
      "Using file dataset/sehat/sehat4-safril.csv\n",
      "0.8778333333333334\n",
      "Using file dataset/sehat/sehat5-hisyam.csv\n",
      "1.1891111111111112\n",
      "Using file dataset/sehat/sehat6-niko.csv\n",
      "0.8545454545454546\n",
      "Using file dataset/sehat/sehat7-aflah.csv\n",
      "1.106\n",
      "Using file dataset/sehat/sehat8-wahyu.csv\n",
      "0.9255384615384615\n",
      "Using file dataset/sehat/sehat9-eric.csv\n",
      "1.0481818181818183\n",
      "Using file dataset/pasien/pasien 2.csv\n",
      "1.334\n",
      "Using file dataset/pasien/pasien 3.csv\n",
      "1.61225\n",
      "Using file dataset/pasien/pasien 4.csv\n",
      "1.1303333333333334\n",
      "Using file dataset/pasien/pasien 5.csv\n",
      "1.1483333333333332\n",
      "Using file dataset/pasien/pasien 6.csv\n",
      "1.025529411764706\n",
      "Using file dataset/pasien/pasien 7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:263: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims, where=where)\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:223: RuntimeWarning: invalid value encountered in true_divide\n",
      "  subok=False)\n",
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:254: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "Using file dataset/pasien/pasien 8.csv\n",
      "1.0761666666666667\n",
      "Using file dataset/pasien/Pasien10-Full.csv\n",
      "1.1444615384615384\n",
      "Using file dataset/pasien/Pasien11-Full.csv\n",
      "1.2372\n",
      "Using file dataset/pasien/Pasien12-Full.csv\n",
      "1.1543333333333332\n",
      "Using file dataset/pasien/Pasien13-Full.csv\n",
      "1.391\n",
      "Using file dataset/pasien/Pasien14-Full.csv\n",
      "0.8322352941176472\n",
      "Using file dataset/pasien/Pasien15-Full.csv\n",
      "1.2545000000000002\n",
      "Using file dataset/pasien/Pasien16-Full.csv\n",
      "2.3093333333333335\n",
      "Using file dataset/pasien/pasien17.csv\n",
      "0.8223333333333332\n",
      "Using file dataset/pasien/pasien18.csv\n",
      "0.8228\n",
      "Using file dataset/pasien/pasien19.csv\n",
      "1.8311428571428572\n",
      "Using file dataset/pasien/pasien20.csv\n",
      "0.9525\n",
      "Using file dataset/pasien/pasien21.csv\n",
      "1.6480000000000001\n",
      "Using file dataset/pasien/pasien22.csv\n",
      "2.6395\n",
      "Using file dataset/pasien/pasien23.csv\n",
      "1.7175999999999998\n",
      "Using file dataset/pasien/pasien24.csv\n",
      "1.2198\n",
      "Using file dataset/pasien/pasien25.csv\n",
      "1.1458333333333333\n",
      "Using file dataset/pasien/pasien26.csv\n",
      "1.2885\n",
      "Using file dataset/pasien/pasien27.csv\n",
      "1.279846153846154\n",
      "Using file dataset/pasien/pasien29.csv\n",
      "0.828\n",
      "Using file dataset/pasien/pasien30.csv\n",
      "1.0270909090909093\n",
      "Using file dataset/pasien/Pasien9-Full.csv\n",
      "2.5660000000000003\n"
     ]
    }
   ],
   "source": [
    "J=0 # jumlah file\n",
    "directory_path = 'dataset/sehat'\n",
    "for iter in range(0,2):\n",
    "    for x in os.listdir(directory_path):\n",
    "        if not x.lower().endswith('.csv'):\n",
    "            continue\n",
    "        J=J+1\n",
    "    directory_path = 'dataset/pasien'\n",
    "n = J #jumlah file\n",
    "m = 5\n",
    "FEAT = [] #bakal jadi Feature.csv\n",
    "for i in range(n): \n",
    "    FEAT.append([0] * m) #mengisi dengan angka 0 semua \n",
    "directory_path = 'dataset/sehat'\n",
    "J=-1\n",
    "K=0\n",
    "FEAT\n",
    "for iter in range(0,2):\n",
    "    for x in os.listdir(directory_path):\n",
    "        if not x.lower().endswith('.csv'):\n",
    "            continue\n",
    "        full_file_path = directory_path  +   '/'   + x\n",
    "        J=J+1\n",
    "        print ('Using file', full_file_path)\n",
    "        try:\n",
    "            dataraw = pd.read_csv(full_file_path,index_col='Timestamp', parse_dates=['Timestamp'])\n",
    "            dataset = pd.DataFrame(dataraw['Value']) #ambil kolom value dari setiap file\n",
    "        except:\n",
    "            dataraw = pd.read_csv(full_file_path,index_col='timestamp', parse_dates=['timestamp'])\n",
    "            dataset = pd.DataFrame(dataraw['values'])\n",
    "        x1=np.array(dataset)  #ubah jadi array, namanya x1\n",
    "        Dat=[]\n",
    "        Dat=[0 for i in range(x1.size)] #bikin array kosong isinya 0 semua sepajang array x1\n",
    "        for i in range(0,x1.size-1):\n",
    "            Dat[i]=max(x1[i]) #why pakai max?\n",
    "        \n",
    "        fs = 110\n",
    "        cutoff_low = 3\n",
    "        powerline=60\n",
    "\n",
    "        # FIR Filter\n",
    "        # The Nyquist rate of the signal.\n",
    "        nyq_rate = fs / 2.0\n",
    "        # The desired width of the transition from pass to stop,\n",
    "        # relative to the Nyquist rate.  We'll design the filter\n",
    "        # with a 5 Hz transition width.\n",
    "        width = 5.0/nyq_rate\n",
    "        # The desired attenuation in the stop band, in dB.\n",
    "        # Compute the order and Kaiser parameter for the FIR filter.\n",
    "        N, beta = kaiserord(powerline, width)\n",
    "        # The cutoff frequency of the filter.\n",
    "        cutoff_hz = cutoff_low\n",
    "        # Use firwin with a Kaiser window to create a lowpass FIR filter.\n",
    "        taps = firwin(N, cutoff_hz/nyq_rate, window=('kaiser', beta))\n",
    "        # Use lfilter to filter x with the FIR filter.\n",
    "        y_filtered = lfilter(taps, 1.0, Dat)\n",
    "       \n",
    "        # FEATURE EXTRACTION\n",
    "        try:\n",
    "            peaks = find_signal_peaks(y_filtered,minimum=0.2)\n",
    "            rr, rr_startstop = get_rr(peaks,to_sec=True,sample_rate=500)\n",
    "            mean_ppi = np.mean(rr)\n",
    "            std_ppi = np.std(rr)\n",
    "            mean_signal = np.mean(y_filtered)\n",
    "            std_signal = np.std(y_filtered)\n",
    "            \n",
    "            FEAT[J][0] = mean_ppi\n",
    "            FEAT[J][1] = std_ppi\n",
    "            FEAT[J][2] = mean_signal\n",
    "            FEAT[J][3] = std_signal\n",
    "            FEAT[J][4] = K\n",
    "            print(FEAT[J][0])\n",
    "            \n",
    "            \n",
    "        except:\n",
    "            J=J-1\n",
    "    directory_path = 'dataset/pasien'\n",
    "    K=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export dataset hasil ekstraksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "#bikin csv\n",
    "with open(\"Feature2.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(FEAT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Feature2.csv', names=['Mean PPI', 'Std PPI', 'Mean Signal','Std signal','Label'])\n",
    "\n",
    "label = dataset['Label']\n",
    "dataset = dataset.drop(columns='Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean PPI</th>\n",
       "      <th>Std PPI</th>\n",
       "      <th>Mean Signal</th>\n",
       "      <th>Std signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.144800</td>\n",
       "      <td>0.376410</td>\n",
       "      <td>0.563133</td>\n",
       "      <td>0.169780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.348206</td>\n",
       "      <td>0.537988</td>\n",
       "      <td>0.180091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.205832</td>\n",
       "      <td>0.457632</td>\n",
       "      <td>0.149985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.775467</td>\n",
       "      <td>0.161472</td>\n",
       "      <td>0.470945</td>\n",
       "      <td>0.170416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.600286</td>\n",
       "      <td>0.652209</td>\n",
       "      <td>0.526859</td>\n",
       "      <td>0.180483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.881667</td>\n",
       "      <td>0.232242</td>\n",
       "      <td>0.540966</td>\n",
       "      <td>0.184731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.923091</td>\n",
       "      <td>0.333139</td>\n",
       "      <td>0.520976</td>\n",
       "      <td>0.199513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.515500</td>\n",
       "      <td>0.806531</td>\n",
       "      <td>0.633792</td>\n",
       "      <td>0.090219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.469333</td>\n",
       "      <td>0.629966</td>\n",
       "      <td>0.594337</td>\n",
       "      <td>0.083619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.469333</td>\n",
       "      <td>0.629966</td>\n",
       "      <td>0.594337</td>\n",
       "      <td>0.083619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.086923</td>\n",
       "      <td>0.457182</td>\n",
       "      <td>0.502952</td>\n",
       "      <td>0.161344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.845273</td>\n",
       "      <td>0.302679</td>\n",
       "      <td>0.528748</td>\n",
       "      <td>0.129080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.918111</td>\n",
       "      <td>0.297846</td>\n",
       "      <td>0.546096</td>\n",
       "      <td>0.112032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.920133</td>\n",
       "      <td>0.319087</td>\n",
       "      <td>0.514343</td>\n",
       "      <td>0.163291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.972000</td>\n",
       "      <td>0.391049</td>\n",
       "      <td>0.603559</td>\n",
       "      <td>0.171897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.982500</td>\n",
       "      <td>0.330631</td>\n",
       "      <td>0.606933</td>\n",
       "      <td>0.166663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.918286</td>\n",
       "      <td>1.283434</td>\n",
       "      <td>0.634110</td>\n",
       "      <td>0.081130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.299400</td>\n",
       "      <td>0.791140</td>\n",
       "      <td>0.569279</td>\n",
       "      <td>0.091696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.928706</td>\n",
       "      <td>0.303369</td>\n",
       "      <td>0.518736</td>\n",
       "      <td>0.146034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.360250</td>\n",
       "      <td>0.717140</td>\n",
       "      <td>0.588101</td>\n",
       "      <td>0.183859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.037500</td>\n",
       "      <td>0.247822</td>\n",
       "      <td>0.594609</td>\n",
       "      <td>0.195016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.993000</td>\n",
       "      <td>0.238510</td>\n",
       "      <td>0.522191</td>\n",
       "      <td>0.146073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.302889</td>\n",
       "      <td>0.427766</td>\n",
       "      <td>0.546934</td>\n",
       "      <td>0.103722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.097778</td>\n",
       "      <td>0.270889</td>\n",
       "      <td>0.605942</td>\n",
       "      <td>0.148605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.195509</td>\n",
       "      <td>0.519471</td>\n",
       "      <td>0.194398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.189111</td>\n",
       "      <td>0.354634</td>\n",
       "      <td>0.539881</td>\n",
       "      <td>0.121469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.854545</td>\n",
       "      <td>0.196541</td>\n",
       "      <td>0.532691</td>\n",
       "      <td>0.195049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.106000</td>\n",
       "      <td>0.367769</td>\n",
       "      <td>0.609578</td>\n",
       "      <td>0.086672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.925538</td>\n",
       "      <td>0.309260</td>\n",
       "      <td>0.484901</td>\n",
       "      <td>0.132912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.048182</td>\n",
       "      <td>0.318662</td>\n",
       "      <td>0.536896</td>\n",
       "      <td>0.148293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.334000</td>\n",
       "      <td>0.405181</td>\n",
       "      <td>0.665749</td>\n",
       "      <td>0.183916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1.612250</td>\n",
       "      <td>0.506913</td>\n",
       "      <td>0.609025</td>\n",
       "      <td>0.175673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.130333</td>\n",
       "      <td>0.597963</td>\n",
       "      <td>0.554111</td>\n",
       "      <td>0.101503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1.148333</td>\n",
       "      <td>0.435033</td>\n",
       "      <td>0.482136</td>\n",
       "      <td>0.242676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.025529</td>\n",
       "      <td>0.457804</td>\n",
       "      <td>0.494655</td>\n",
       "      <td>0.244294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.682579</td>\n",
       "      <td>0.081010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.076167</td>\n",
       "      <td>0.409362</td>\n",
       "      <td>0.548745</td>\n",
       "      <td>0.181870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.144462</td>\n",
       "      <td>0.462427</td>\n",
       "      <td>0.558760</td>\n",
       "      <td>0.229885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1.237200</td>\n",
       "      <td>0.531010</td>\n",
       "      <td>0.601591</td>\n",
       "      <td>0.134306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.154333</td>\n",
       "      <td>0.408424</td>\n",
       "      <td>0.554974</td>\n",
       "      <td>0.208372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.391000</td>\n",
       "      <td>0.634977</td>\n",
       "      <td>0.607175</td>\n",
       "      <td>0.185317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.832235</td>\n",
       "      <td>0.217524</td>\n",
       "      <td>0.540464</td>\n",
       "      <td>0.228747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.254500</td>\n",
       "      <td>0.201789</td>\n",
       "      <td>0.604682</td>\n",
       "      <td>0.126000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.309333</td>\n",
       "      <td>1.767223</td>\n",
       "      <td>0.596359</td>\n",
       "      <td>0.181810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.822333</td>\n",
       "      <td>0.271768</td>\n",
       "      <td>0.548211</td>\n",
       "      <td>0.108535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.822800</td>\n",
       "      <td>0.306241</td>\n",
       "      <td>0.527910</td>\n",
       "      <td>0.176042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.831143</td>\n",
       "      <td>0.571829</td>\n",
       "      <td>0.598152</td>\n",
       "      <td>0.094912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.952500</td>\n",
       "      <td>0.238610</td>\n",
       "      <td>0.523302</td>\n",
       "      <td>0.113137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1.648000</td>\n",
       "      <td>0.529244</td>\n",
       "      <td>0.665612</td>\n",
       "      <td>0.123959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2.639500</td>\n",
       "      <td>0.900732</td>\n",
       "      <td>0.743923</td>\n",
       "      <td>0.134725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.717600</td>\n",
       "      <td>0.469675</td>\n",
       "      <td>0.653050</td>\n",
       "      <td>0.155324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.219800</td>\n",
       "      <td>0.453792</td>\n",
       "      <td>0.617893</td>\n",
       "      <td>0.196630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.145833</td>\n",
       "      <td>0.338302</td>\n",
       "      <td>0.549298</td>\n",
       "      <td>0.196509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.288500</td>\n",
       "      <td>0.554390</td>\n",
       "      <td>0.586477</td>\n",
       "      <td>0.085139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.279846</td>\n",
       "      <td>0.472758</td>\n",
       "      <td>0.542772</td>\n",
       "      <td>0.189468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.259534</td>\n",
       "      <td>0.559435</td>\n",
       "      <td>0.171581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.027091</td>\n",
       "      <td>0.310055</td>\n",
       "      <td>0.579883</td>\n",
       "      <td>0.170939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2.566000</td>\n",
       "      <td>2.960521</td>\n",
       "      <td>0.643660</td>\n",
       "      <td>0.131047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Mean PPI   Std PPI  Mean Signal  Std signal\n",
       "0   1.144800  0.376410     0.563133    0.169780\n",
       "1   0.917500  0.348206     0.537988    0.180091\n",
       "2   0.828000  0.205832     0.457632    0.149985\n",
       "3   0.775467  0.161472     0.470945    0.170416\n",
       "4   1.600286  0.652209     0.526859    0.180483\n",
       "5   0.881667  0.232242     0.540966    0.184731\n",
       "6   0.923091  0.333139     0.520976    0.199513\n",
       "7   1.515500  0.806531     0.633792    0.090219\n",
       "8   1.469333  0.629966     0.594337    0.083619\n",
       "9   1.469333  0.629966     0.594337    0.083619\n",
       "10  1.086923  0.457182     0.502952    0.161344\n",
       "11  0.845273  0.302679     0.528748    0.129080\n",
       "12  0.918111  0.297846     0.546096    0.112032\n",
       "13  0.920133  0.319087     0.514343    0.163291\n",
       "14  0.972000  0.391049     0.603559    0.171897\n",
       "15  0.982500  0.330631     0.606933    0.166663\n",
       "16  1.918286  1.283434     0.634110    0.081130\n",
       "17  1.299400  0.791140     0.569279    0.091696\n",
       "18  0.928706  0.303369     0.518736    0.146034\n",
       "19  1.360250  0.717140     0.588101    0.183859\n",
       "20  1.037500  0.247822     0.594609    0.195016\n",
       "21  0.993000  0.238510     0.522191    0.146073\n",
       "22  1.302889  0.427766     0.546934    0.103722\n",
       "23  1.097778  0.270889     0.605942    0.148605\n",
       "24  0.877833  0.195509     0.519471    0.194398\n",
       "25  1.189111  0.354634     0.539881    0.121469\n",
       "26  0.854545  0.196541     0.532691    0.195049\n",
       "27  1.106000  0.367769     0.609578    0.086672\n",
       "28  0.925538  0.309260     0.484901    0.132912\n",
       "29  1.048182  0.318662     0.536896    0.148293\n",
       "30  1.334000  0.405181     0.665749    0.183916\n",
       "31  1.612250  0.506913     0.609025    0.175673\n",
       "32  1.130333  0.597963     0.554111    0.101503\n",
       "33  1.148333  0.435033     0.482136    0.242676\n",
       "34  1.025529  0.457804     0.494655    0.244294\n",
       "35       NaN       NaN     0.682579    0.081010\n",
       "36  1.076167  0.409362     0.548745    0.181870\n",
       "37  1.144462  0.462427     0.558760    0.229885\n",
       "38  1.237200  0.531010     0.601591    0.134306\n",
       "39  1.154333  0.408424     0.554974    0.208372\n",
       "40  1.391000  0.634977     0.607175    0.185317\n",
       "41  0.832235  0.217524     0.540464    0.228747\n",
       "42  1.254500  0.201789     0.604682    0.126000\n",
       "43  2.309333  1.767223     0.596359    0.181810\n",
       "44  0.822333  0.271768     0.548211    0.108535\n",
       "45  0.822800  0.306241     0.527910    0.176042\n",
       "46  1.831143  0.571829     0.598152    0.094912\n",
       "47  0.952500  0.238610     0.523302    0.113137\n",
       "48  1.648000  0.529244     0.665612    0.123959\n",
       "49  2.639500  0.900732     0.743923    0.134725\n",
       "50  1.717600  0.469675     0.653050    0.155324\n",
       "51  1.219800  0.453792     0.617893    0.196630\n",
       "52  1.145833  0.338302     0.549298    0.196509\n",
       "53  1.288500  0.554390     0.586477    0.085139\n",
       "54  1.279846  0.472758     0.542772    0.189468\n",
       "55  0.828000  0.259534     0.559435    0.171581\n",
       "56  1.027091  0.310055     0.579883    0.170939\n",
       "57  2.566000  2.960521     0.643660    0.131047"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mean PPI       1\n",
       "Std PPI        1\n",
       "Mean Signal    0\n",
       "Std signal     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Mean PPI'] = dataset['Mean PPI'].fillna(0)\n",
    "dataset['Std PPI'] = dataset['Std PPI'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tahapan Klasifikasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "DATA TRAIN\n",
      "(40, 4)\n",
      "LABEL TRAIN\n",
      "(40,)\n",
      "DATA TEST\n",
      "(18, 4)\n",
      "LABEL TEST\n",
      "(18,)\n",
      "HASIL PREDIKSI\n",
      "[1 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1]\n",
      "The training accuracy is 1.0\n",
      "The test accuracy is 0.8333333333333334\n",
      "HASIL PREDIKSI\n",
      "[1 1 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0]\n",
      "Akurasi SVM 72.22222222222221  %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helmi.ruslan\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py:163: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y_encoded = np.zeros(y.shape, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "# MACHINE LEARNING\n",
    "# SPLIT DATA 80% TRAIN, 20% DATA TEST\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from numpy import random\n",
    "\n",
    "x = random.randint(100)\n",
    "print(x)\n",
    "\n",
    "# 77,83,17\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.30,random_state=67)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "print('DATA TRAIN')\n",
    "print(X_train.shape)\n",
    "print('LABEL TRAIN')\n",
    "print(y_train.shape)\n",
    "print('DATA TEST')\n",
    "print(X_test.shape)\n",
    "print('LABEL TEST')\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "# Import the classifier from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: Define the classifier, and fit it to the data\n",
    "model = DecisionTreeClassifier(random_state=20)\n",
    "model.fit(X_train, y_train) \n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "print('HASIL PREDIKSI')\n",
    "print(y_test_pred)\n",
    "y_train_pred = model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "cm2 = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print('The training accuracy is', train_accuracy)\n",
    "print('The test accuracy is', test_accuracy)\n",
    "\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('HASIL PREDIKSI')\n",
    "print(y_pred)\n",
    "67\n",
    "83\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ac = accuracy_score(y_test,y_pred)\n",
    "print (\"Akurasi SVM\", ac*100,' %') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm[1][1]\n",
    "TN = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = cm2[1][1]\n",
    "TN = cm2[0][0]\n",
    "FP = cm2[0][1]\n",
    "FN = cm2[1][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hasil Sensitiviti, spesifisiti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n",
      "0.9090909090909091\n",
      "Specificity: 0.91\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "conf_accuracy = (float (TP+TN) / float(TP + TN + FP + FN))\n",
    "\n",
    "# calculate mis-classification\n",
    "conf_misclassification = 1- conf_accuracy\n",
    "\n",
    "# calculate the sensitivity\n",
    "conf_sensitivity = (TP / float(TP + FN))\n",
    "# calculate the specificity\n",
    "conf_specificity = (TN / float(TN + FP))\n",
    "\n",
    "# calculate precision\n",
    "conf_precision = (TN / float(TN + FP))\n",
    "print(conf_sensitivity)\n",
    "print(conf_specificity)\n",
    "print(f'Specificity: {round(conf_specificity,2)}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=[[29492,12649], [10847,47012]]\n",
    "Accuracy :  0.76504\n",
    "Sensitivity :  0.699841009943\n",
    "Specificity :  0.812527005306"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.clf()\n",
    "plot_with_rpeaks(signal,peaks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.clf()\n",
    "plot_with_rpeaks(signal,peaks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.clf()\n",
    "plot_with_rpeaks(ppg_clean,peaks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_rpeaks(filtered,r_peaks):\n",
    "    peaks = [filtered[peak] for peak in r_peaks]\n",
    "    plt.plot(filtered)\n",
    "    plt.scatter(r_peaks,peaks,c='red')\n",
    "#    plt.savefig(\"sinyal_clean_peaks\",dpi=1000)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_clean = nk.ppg_clean(Dat)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
